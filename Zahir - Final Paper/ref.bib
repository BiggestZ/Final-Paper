
@online{tripathi_how_2023,
	title = {How to Evaluate a Large Language Model ({LLM})?},
	url = {https://www.analyticsvidhya.com/blog/2023/05/how-to-evaluate-a-large-language-model-llm/},
	abstract = {Learn how to use different evaluation frameworks and techniques for Large Language Models, so you can select the right {LLM} for your project.},
	titleaddon = {Analytics Vidhya},
	author = {Tripathi, Gyan Prakash},
	urldate = {2024-05-08},
	date = {2023-05-16},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/F7NNQTDQ/how-to-evaluate-a-large-language-model-llm.html:text/html},
}

@online{greyling_what_2023,
	title = {What Are {LLMs} Good At \& When Can {LLMs} Fail?},
	url = {https://cobusgreyling.medium.com/what-are-llms-good-at-when-can-llms-fail-9e4eaf7ec723},
	abstract = {As seen in the graph above, the number of Large Language Model ({LLM}) related studies are increasing immensely. Considering some of these…},
	titleaddon = {Medium},
	author = {Greyling, Cobus},
	urldate = {2024-05-08},
	date = {2023-11-06},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/TSK942VL/what-are-llms-good-at-when-can-llms-fail-9e4eaf7ec723.html:text/html},
}

@online{noauthor_number_2023,
	title = {Number of {ChatGPT} Users (May 2024)},
	url = {https://explodingtopics.com/blog/chatgpt-users},
	abstract = {Statistics and growth data related to the number of {ChatGPT} users.},
	titleaddon = {Exploding Topics},
	urldate = {2024-05-08},
	date = {2023-03-30},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/DUI49RR6/chatgpt-users.html:text/html},
}

@online{lammertyn_60_nodate,
	title = {60+ {ChatGPT} Statistics And Facts You Need to Know in 2024},
	url = {https://blog.invgate.com/chatgpt-statistics},
	abstract = {From achieving 100 million active users in just two months to receiving \$10 billion from Microsoft, here are all {ChatGPT} statistics and facts you need to know.},
	author = {Lammertyn, Marina},
	urldate = {2024-05-08},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/EYWT3YGW/chatgpt-statistics.html:text/html},
}

@online{gandhi_evaluation_2020,
	title = {Evaluation of Language Models through Perplexity and Shannon Visualization Method},
	url = {https://towardsdatascience.com/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0},
	abstract = {How good is your language model?},
	titleaddon = {Medium},
	author = {Gandhi, Meet},
	urldate = {2024-05-08},
	date = {2020-03-31},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/4JPPBEXT/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0.html:text/html},
}

@online{priyanka_perplexity_2022,
	title = {Perplexity of Language Models},
	url = {https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72},
	abstract = {Perplexity is an evaluation metric that measures the quality of language models. In this post, we will discuss what perplexity is and how…},
	titleaddon = {Medium},
	author = {Priyanka},
	urldate = {2024-05-08},
	date = {2022-11-26},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/MRREUG4X/perplexity-of-language-models-41160427ed72.html:text/html},
}

@online{laumann_word_2023,
	title = {Word Error Rate 101: Your Guide to {STT} Vendor Evaluation},
	url = {https://medium.com/neuralspace/word-error-rate-101-your-guide-to-stt-vendor-evaluation-5b68072fcbf7},
	shorttitle = {Word Error Rate 101},
	abstract = {In the rapidly evolving world of Speech-to-Text ({STT}) technology, making an informed choice can seem overwhelming. Yet, the success of your…},
	titleaddon = {{NeuralSpace}},
	author = {Laumann, Felix},
	urldate = {2024-05-09},
	date = {2023-11-07},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/RMLF9K59/word-error-rate-101-your-guide-to-stt-vendor-evaluation-5b68072fcbf7.html:text/html},
}

@article{benyou_csc6203cie6021_nodate,
	title = {{CSC}6203/{CIE}6021:   Large Language Model    Lecture 4: Training {LLMs} from scratch},
	author = {Benyou, {WANG}},
	langid = {english},
	file = {Benyou - CSC6203CIE6021   Large Language Model    Lecture.pdf:/Users/Zahir/Zotero default/storage/WRKMPKS3/Benyou - CSC6203CIE6021   Large Language Model    Lecture.pdf:application/pdf},
}

@online{noauthor_httpsllm-coursegithubiomaterialslecture4-trainingllmspdf_nodate,
	title = {https://llm-course.github.io/materials/Lecture4-{TrainingLLMs}.pdf},
	url = {https://llm-course.github.io/materials/Lecture4-TrainingLLMs.pdf},
	urldate = {2024-05-09},
}

@online{noauthor_pre-training_nodate,
	title = {Pre-training in {LLM} Development},
	url = {https://toloka.ai/blog/pre-training-in-llm-development/},
	abstract = {Why is it so critical to pre-train {AI} models, and {LLMs} in particular? And what other indispensable steps should be taken to obtain effective language models that comprehend user preferences and can fulfill your business-specific tasks?},
	titleaddon = {Pre-training in {LLM} Development},
	urldate = {2024-05-09},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/R79IPSMS/pre-training-in-llm-development.html:text/html},
}

@online{noauthor_25_nodate,
	title = {(25) Teaching An Old Dog New Tricks: The Difference Between Fine-Tuning and Pre-Training {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/pulse/teaching-old-dog-new-tricks-difference-between-lewis-ms-ccrp-ches-3abqc/},
	urldate = {2024-05-09},
	file = {(25) Teaching An Old Dog New Tricks\: The Difference Between Fine-Tuning and Pre-Training | LinkedIn:/Users/Zahir/Zotero default/storage/3C2LN4WT/teaching-old-dog-new-tricks-difference-between-lewis-ms-ccrp-ches-3abqc.html:text/html},
}

@online{noauthor_what_2024,
	title = {What Is Reinforcement Learning From Human Feedback ({RLHF})? {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/rlhf},
	shorttitle = {What Is Reinforcement Learning From Human Feedback ({RLHF})?},
	abstract = {Reinforcement learning from human feedback ({RLHF}) is a machine learning technique in which a “reward model” is trained by human feedback to optimize an {AI} agent},
	urldate = {2024-05-09},
	date = {2024-04-15},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/VWN5BL7U/rlhf.html:text/html},
}

@article{radford_improving_nodate,
	title = {Improving Language Understanding by Generative Pre-Training},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	langid = {english},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/Zahir/Zotero default/storage/J945VZ9X/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{radford_language_nodate,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	langid = {english},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/Zahir/Zotero default/storage/TT78KHCM/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@online{noauthor_zero-shot_nodate,
	title = {Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning: A technical walkthrough using {OpenAI}'s {APIs} \& models},
	url = {https://labelbox-guides.ghost.io/zero-shot-learning-few-shot-learning-fine-tuning/},
	shorttitle = {Zero-Shot Learning vs. Few-Shot Learning vs. Fine-Tuning},
	abstract = {In this guide, we’ll walk through the key difference between these techniques and how to implement them. Explore how to use each of these learning techniques with Labelbox’s {LLM} Editor \& Labelbox Model},
	urldate = {2024-05-09},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/XN2RERPN/zero-shot-learning-few-shot-learning-fine-tuning.html:text/html},
}

@online{leswing_chatgpt_2023,
	title = {{ChatGPT} and generative {AI} are booming, but the costs can be extraordinary},
	url = {https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html},
	abstract = {It can cost millions of dollars to train and operate generative {AI} technologies like {ChatGPT}, which are being subsidized by tech companies and {VCs}.},
	titleaddon = {{CNBC}},
	author = {Leswing, Kif, Jonathan Vanian},
	urldate = {2024-05-09},
	date = {2023-03-13},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/MD5ZJABW/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html:text/html},
}

@online{american_express_comprehensive_2019,
	title = {A Comprehensive Guide to Attention Mechanism in Deep Learning for Everyone},
	url = {https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/},
	abstract = {A complete guide to attention models and attention mechanisms in deep learning. Learn how to implement an attention model in python using keras.},
	titleaddon = {Analytics Vidhya},
	author = {american{\textbackslash}\_express},
	urldate = {2024-05-09},
	date = {2019-11-20},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/37JELQJA/comprehensive-guide-attention-mechanism-deep-learning.html:text/html},
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - Multi-Head Attention Explained},
	url = {https://paperswithcode.com/method/multi-head-attention},
	abstract = {Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). 

\$\$ {\textbackslash}text\{{MultiHead}\}{\textbackslash}left({\textbackslash}textbf\{Q\}, {\textbackslash}textbf\{K\}, {\textbackslash}textbf\{V\}{\textbackslash}right) = {\textbackslash}left[{\textbackslash}text\{head\}\_\{1\},{\textbackslash}dots,{\textbackslash}text\{head\}\_\{h\}{\textbackslash}right]{\textbackslash}textbf\{W\}\_\{0\}\$\$

\$\${\textbackslash}text\{where\} {\textbackslash}text\{ head\}\_\{i\} = {\textbackslash}text\{Attention\} {\textbackslash}left({\textbackslash}textbf\{Q\}{\textbackslash}textbf\{W\}\_\{i\}{\textasciicircum}\{Q\}, {\textbackslash}textbf\{K\}{\textbackslash}textbf\{W\}\_\{i\}{\textasciicircum}\{K\}, {\textbackslash}textbf\{V\}{\textbackslash}textbf\{W\}\_\{i\}{\textasciicircum}\{V\} {\textbackslash}right) \$\$

Above \${\textbackslash}textbf\{W\}\$ are all learnable parameter matrices.

Note that scaled dot-product attention is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.

Source: Lilian Weng},
	urldate = {2024-05-09},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/SLWM26HE/multi-head-attention.html:text/html},
}

@online{noauthor_what_2023,
	title = {What is Grouped Query Attention ({GQA})? — Klu},
	url = {https://klu.ai/glossary/grouped-query-attention},
	shorttitle = {What is Grouped Query Attention ({GQA})?},
	abstract = {Grouped Query Attention ({GQA}) is a technique used in large language models to speed up the inference time. It groups queries together and computes their attention jointly, reducing the computational complexity and making the model more efficient.},
	urldate = {2024-05-09},
	date = {2023-07-04},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/LK6PTYLW/grouped-query-attention.html:text/html},
}

@online{noauthor_papers_nodate-1,
	title = {Papers with Code - Sliding Window Attention Explained},
	url = {https://paperswithcode.com/method/sliding-window-attention},
	abstract = {Sliding Window Attention is an attention pattern for attention-based models. It was proposed as part of the Longformer architecture. It is motivated by the fact that non-sparse attention in the original Transformer formulation has a self-attention component with \$O{\textbackslash}left(n{\textasciicircum}\{2\}{\textbackslash}right)\$ time and memory complexity where \$n\$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input. 

More formally, in this attention pattern, given a fixed window size \$w\$, each token attends to \${\textbackslash}frac\{1\}\{2\}w\$ tokens on each side. The computation complexity of this pattern is \$O{\textbackslash}left(n×w{\textbackslash}right)\$,
which scales linearly with input sequence length \$n\$. To make this attention pattern efficient, \$w\$ should be small compared with \$n\$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to {CNNs} where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)

In this case, with a transformer of \$l\$ layers, the receptive field size is \$l × w\$ (assuming
\$w\$ is fixed for all layers). Depending on the application, it might be helpful to use different values of \$w\$ for each layer to balance between efficiency and model representation capacity.},
	urldate = {2024-05-09},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/5FSM67WN/sliding-window-attention.html:text/html},
}

@online{gathnex_mistral-7b_2023,
	title = {Mistral-7B Fine-Tuning: A Step-by-Step Guide},
	url = {https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8},
	shorttitle = {Mistral-7B Fine-Tuning},
	abstract = {Introducing Mistral 7B: The Powerhouse of Language Models},
	titleaddon = {Medium},
	author = {Gathnex},
	urldate = {2024-05-09},
	date = {2023-11-25},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/B6ECBY8A/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8.html:text/html},
}

@online{noauthor_how_nodate,
	title = {How long does it typically take to train a dataset for a single {PDF} when fine-tuning a model with Azure {AI}? - Microsoft Q\&A},
	url = {https://learn.microsoft.com/en-us/answers/questions/1528058/how-long-does-it-typically-take-to-train-a-dataset},
	shorttitle = {How long does it typically take to train a dataset for a single {PDF} when fine-tuning a model with Azure {AI}?},
	abstract = {I am planning to finetune {GPT}-3.5 Turbo with Azure {AI}.
I have a {PDF} document consisting of 10 pages, containing approximately 17,000 characters and 3,200 tokens. Now, I intend to fine-tune it using {GPT}-3.5 Turbo. How long will it actually take to train…},
	urldate = {2024-05-09},
	langid = {english},
	file = {Snapshot:/Users/Zahir/Zotero default/storage/7C3KA9QS/how-long-does-it-typically-take-to-train-a-dataset.html:text/html},
}
